{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Efficiently deploy a Sentence Transformer model with optimum and AWS SageMaker Serverless Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this end-to-end tutorial, you will:\n",
    "\n",
    "1. [Convert a sentence transformer model to ONNX with Optimum.](#1-convert-a-sentence-transformer-model-to-onnx-with-optimum)\n",
    "2. [Create a custom inference script for the SageMaker endpoint.](#2-create-a-custom-inference-script-for-the-sagemaker-endpoint)\n",
    "3. [Create an AWS Role with the necessary permissions.](#3-create-an-aws-role-with-the-necessary-permissions)\n",
    "4. [Upload all necessary files to S3.](#4-upload-all-necessary-files-to-s3)\n",
    "5. [Create a SageMaker model and serverless endpoint.](#5-create-a-sagemaker-model-and-serverless-endpoint)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This code is meant to be run locally, if you want to run it within an AWS managed environment, e.g as a SageMaker Studio Notebook, the step 3 would look a bit different, but the rest should remain very similar or the same.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1-convert-a-sentence-transformer-model-to-onnx-with-optimum'></a>\n",
    "## 1. Convert a sentence transformer model to ONNX with [Optimum](https://github.com/huggingface/optimum)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load the desired model using the `ORTModelForFeatureExtraction` class, the `export` parameter tells it that is a transformers model so it can load it properly and convert it to ONNX. We will also need the tokenizer so we load it and save to the same directory `onnx_path` as with the converted model.\n",
    "\n",
    "In this case, we are not trying to optimize the final ONNX model, but several things can be done to squeeze more inferences per second to the final model, such as graph optimization and/or dynamic quantization. If you want to know more about it, you can [check this great article about it.](https://www.philschmid.de/optimize-sentence-transformers)\n",
    "\n",
    "This tutorial uses a [pretrained model from huggingface hub](https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2), but if you have a fined tuned model, just replace the value in `model_id` with the path to your local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
    "\n",
    "model_id = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "onnx_path = \"tmp\"\n",
    "\n",
    "# load vanilla transformers and convert to onnx\n",
    "model = ORTModelForFeatureExtraction.from_pretrained(model_id, export=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# save onnx checkpoint and tokenizer\n",
    "model.save_pretrained(onnx_path)\n",
    "tokenizer.save_pretrained(onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2-create-a-custom-inference-script-for-the-sagemaker-endpoint'></a>\n",
    "## 2. Create a custom inference script for the SageMaker endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For creating this inference endpoint, we are going to use the [SageMaker Hugging Face Inference Toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit), and because we are deploying a sentence transformer model, we need a way to specify how the inferences need to be performed, we do this by creating an `inference.py` script. In this case we need to create three functions inside that script.\n",
    "\n",
    "* `model_fn` - this received the path to the model directory and outputs the model and tokenizer.\n",
    "* `predict_fn` - this receives the inputs and the output from model_fn and outputs the predictions.\n",
    "* `mean_pooling` - this one is just a helper function, as we need a way to calculate the mean pooling over the outputs of the model.\n",
    "\n",
    "The inference toolkit expects this script to be located inside a directory named `code`, so we create it and save the custom script inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/inference.py\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
    "\n",
    "model_name = 'model' # This has to be the same as the one inside onnx_path\n",
    "\n",
    "# Helper: Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(model_output.size()).float()\n",
    "    return torch.sum(model_output * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    # load tokenizer and neuron model from model_dir\n",
    "    model = ORTModelForFeatureExtraction.from_pretrained(model_dir, file_name=f\"{model_name}.onnx\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def predict_fn(data, model_tokenizer_model_config):\n",
    "    # destruct model and tokenizer\n",
    "    model, tokenizer = model_tokenizer_model_config\n",
    "\n",
    "    # Tokenize sentences\n",
    "    inputs = data.pop(\"inputs\", data)\n",
    "    encoded_inputs = tokenizer(inputs, padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_outputs = model(**encoded_inputs)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_outputs[\"last_hidden_state\"], encoded_inputs['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    # return dictonary, which will be json serializable\n",
    "    return {\"vectors\": sentence_embeddings.tolist()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are using additional libraries for our custom inference script, we need also to create a `requirements.txt` indicating the libraries that need to be installed for the inferences to run properly, in our case optimum with ONNX support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/requirements.txt\n",
    "\n",
    "optimum[onnxruntime]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3-create-an-aws-role-with-the-necessary-permissions'></a>\n",
    "## 3. Create an AWS Role with the necessary permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a Role that has all the required permissions to perform all the necessary actions to deploy this endpoint, you first need to go to **IAM -> Roles -> Create Role**. Once you're there, select the **AWS Account** option and go next, then on the Add permissions step, search for `AmazonSageMakerFullAccess`, select it and go next, finally give it a name and create the role. In this tutorial we will be using an AWS user to assume this role and to do so we need to edit this role's trust policy. To do this, go to trust relationships and edit it to give your desired user the permission to assume this role. The entire JSON should be something like the one below, just replace **arn:aws:iam::XXXXXXXXX:user/username** with your user ARN.\n",
    "\n",
    "```\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"sagemaker.amazonaws.com\",\n",
    "                \"AWS\": \"arn:aws:iam::XXXXXXXXX:user/username\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {}\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally to make sure your code will use the correct role and user, create a `.env` file and add the following environment variables with the correct information:\n",
    "* AWS_ACCESS_KEY_ID\n",
    "* AWS_SECRET_ACCESS_KEY\n",
    "* AWS_DEFAULT_REGION\n",
    "* AWS_ROLE_ARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a sagemaker session using the previously setup role and use it to deploy our endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "role_arn = os.environ[\"AWS_ROLE_ARN\"]\n",
    "\n",
    "session = boto3.Session()\n",
    "sts = session.client(\"sts\")\n",
    "response = sts.assume_role(\n",
    "    RoleArn=role_arn,\n",
    "    RoleSessionName=\"sagemaker-test\"\n",
    ")\n",
    "\n",
    "boto_session = boto3.Session(\n",
    "    aws_access_key_id=response['Credentials']['AccessKeyId'],\n",
    "    aws_secret_access_key=response['Credentials']['SecretAccessKey'],\n",
    "    aws_session_token=response['Credentials']['SessionToken']\n",
    ")\n",
    "\n",
    "sess = sagemaker.Session(boto_session=boto_session)\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "\n",
    "sess = sagemaker.Session(boto_session=boto_session, default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role_arn}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4-upload-all-necessary-files-to-s3'></a>\n",
    "## 4. Upload all necessary files to S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our SageMaker model, first we will need to create a `model.tar.gz` file with all the necessary model files and the custom inference code, and upload that file to a S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r code/ tmp/code/\n",
    "\n",
    "%cd tmp\n",
    "!tar zcvf model.tar.gz *\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# create s3 uri\n",
    "s3_model_path = f\"s3://{sess.default_bucket()}/onnx\"\n",
    "\n",
    "# upload model.tar.gz\n",
    "s3_model_uri = S3Uploader.upload(local_path=\"tmp/model.tar.gz\",desired_s3_uri=s3_model_path, sagemaker_session=sess)\n",
    "print(f\"model artifcats uploaded to {s3_model_uri}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5-create-a-sagemaker-model-and-serverless-endpoint'></a>\n",
    "## 5. Create a SageMaker model and serverless endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can leverage the [HuggingFaceModel](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-model) class to create the SageMaker model. As it is our intention to deploy this model as a serverless endpoint, we need to use a [ServerlessInferenceConfig](https://sagemaker.readthedocs.io/en/v2.203.0/api/inference/serverless.html) to configure the endpoint to our needs. Then we only need to run the model's `deploy` method and pass the configuration object. This step may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "from sagemaker.serverless.serverless_inference_config import ServerlessInferenceConfig\n",
    "\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=s3_model_uri,       # path to your model and script\n",
    "   role=role_arn,                 # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.12\",   # transformers version used\n",
    "   pytorch_version=\"1.9\",         # pytorch version used\n",
    "   py_version='py38',             # python version used\n",
    "   sagemaker_session=sess\n",
    ")\n",
    "\n",
    "\n",
    "serverless_config = ServerlessInferenceConfig(\n",
    "    memory_size_in_mb=3072,\n",
    "    max_concurrency=1,\n",
    ")\n",
    "\n",
    "predictor = huggingface_model.deploy(serverless_inference_config=serverless_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is up and running we can start running inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictor.predict({\"inputs\": [\"this is a test text\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean up, we can delete the model and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well Done! In this tutorial you learned how to convert a Sentence Transformers model to ONNX and how to deploy that converted model as a SageMaker Serverless Inference Endpoint.\n",
    "Further steps may include testing different ONNX optimizations and/or different endpoint configurations to find the most suitable setting for your scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "1. [Accelerate Sentence Transformers with Hugging Face Optimum](https://www.philschmid.de/optimize-sentence-transformers)\n",
    "2. [Accelerated document embeddings with Hugging Face Transformers and AWS Inferentia](https://www.philschmid.de/huggingface-sentence-transformers-aws-inferentia#2-create-a-custom-inferencepy-script-for-sentence-embeddings)\n",
    "3. [SageMaker Serverless Inference](https://github.com/aws/amazon-sagemaker-examples/blob/main/serverless-inference/huggingface-serverless-inference/huggingface-text-classification-serverless-inference.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
